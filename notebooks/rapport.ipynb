{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef43970b",
   "metadata": {},
   "source": [
    "# Prévision de consommation électrique — SARIMA vs LSTM (H=24)\n",
    "\n",
    "## Introduction\n",
    "La consommation électrique présente des cycles forts (journée, semaine, saison) et des variations dues à des facteurs externes (température, calendrier). Pouvoir anticiper la charge à 24h est un cas d’usage classique pour :\n",
    "\n",
    "- planifier la production et les achats d’énergie,\n",
    "- réduire les coûts (pénalités, recours à des moyens de production coûteux),\n",
    "- mieux gérer les pics de consommation.\n",
    "\n",
    "Ce notebook présente une démarche simple et reproductible :\n",
    "\n",
    "1. mise en place du projet et chargement des données,\n",
    "2. analyse exploratoire (EDA),\n",
    "3. entraînement et évaluation de deux approches de prévision multi-horizon (H=24) :\n",
    "   - SARIMA/SARIMAX (statistique, saisonnalité journalière)\n",
    "   - LSTM (deep learning, prédiction directe du vecteur 24h)\n",
    "4. comparaison des performances (MAE/RMSE/MAPE), visualisations (true vs pred) et analyse d’erreurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb3fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Résolution robuste de la racine du repo pour que `import src...` marche\n",
    "CWD = Path.cwd()\n",
    "if (CWD / \"src\").exists():\n",
    "    REPO_ROOT = CWD\n",
    "elif (CWD.parent / \"src\").exists():\n",
    "    REPO_ROOT = CWD.parent\n",
    "else:\n",
    "    REPO_ROOT = CWD\n",
    "\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from src.data_processing import (\n",
    "    add_lag_features,\n",
    "    add_time_features,\n",
    "    load_consumption_data,\n",
    "    load_opsd_weather,\n",
    "    merge_weather_features,\n",
    "    preprocess_household_hourly,\n",
    ")\n",
    "from src.evaluate import evaluate_forecasts, sarima_rolling_forecast\n",
    "from src.models import LSTMForecaster\n",
    "from src.train import make_sequences\n",
    "from src.utils import temporal_split\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.width = 140\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c82229",
   "metadata": {},
   "source": [
    "## 1) Chargement des données\n",
    "\n",
    "On utilise les CSV présents dans `data/` (OPSD). Tu peux changer `COUNTRY`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e997ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "consumption_path = DATA_DIR / \"consumption_data.csv\"\n",
    "weather_path = DATA_DIR / \"weather_data.csv\"\n",
    "\n",
    "assert consumption_path.exists(), f\"Missing {consumption_path}\"\n",
    "assert weather_path.exists(), f\"Missing {weather_path}\"\n",
    "\n",
    "COUNTRY = \"FR\"\n",
    "HORIZON = 24\n",
    "INPUT_WIDTH = 24\n",
    "\n",
    "raw = load_consumption_data(str(consumption_path), country=COUNTRY)\n",
    "hourly = preprocess_household_hourly(raw)\n",
    "\n",
    "# Features: calendrier + lags/rolling + température\n",
    "feat = add_time_features(hourly, holidays_country=COUNTRY)\n",
    "feat = add_lag_features(feat, target_col=\"load\", lags=(1, 24), rolling_windows=(3, 24), dropna=True)\n",
    "\n",
    "temp = load_opsd_weather(\n",
    "    str(weather_path),\n",
    "    datetime_col=\"utc_timestamp\",\n",
    "    temperature_col=f\"{COUNTRY}_temperature\",\n",
    ")\n",
    "feat = merge_weather_features(feat, temp)\n",
    "feat = feat.dropna(subset=[\"load\"])  # sécurité\n",
    "\n",
    "feat.head(), feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bbbb5f",
   "metadata": {},
   "source": [
    "## 2) Analyse exploratoire (EDA)\n",
    "\n",
    "On visualise : tendance globale, profil horaire, effet weekend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = feat[\"load\"].rename(\"load\")\n",
    "\n",
    "global_daily = series.resample(\"D\").mean()\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "global_daily.plot(ax=ax, color=\"tab:blue\", linewidth=1)\n",
    "ax.set_title(f\"{COUNTRY} — Consommation moyenne quotidienne\")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Load\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "hod = series.groupby(series.index.hour).mean()\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "hod.plot(kind=\"bar\", ax=ax, color=\"tab:blue\")\n",
    "ax.set_title(f\"{COUNTRY} — Profil moyen par heure\")\n",
    "ax.set_xlabel(\"Heure\")\n",
    "ax.set_ylabel(\"Load\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "tmp = series.to_frame()\n",
    "tmp[\"hour\"] = tmp.index.hour\n",
    "tmp[\"is_weekend\"] = tmp.index.weekday >= 5\n",
    "pivot = tmp.groupby([\"is_weekend\", \"hour\"]).mean().unstack(0)[\"load\"]\n",
    "pivot.columns = [\"Semaine\", \"Weekend\"]\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "pivot.plot(ax=ax)\n",
    "ax.set_title(f\"{COUNTRY} — Profil horaire : semaine vs weekend\")\n",
    "ax.set_xlabel(\"Heure\")\n",
    "ax.set_ylabel(\"Load\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f5376",
   "metadata": {},
   "source": [
    "## 3) Split temporel (train/val/test)\n",
    "\n",
    "Découpage chronologique 70/15/15.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20318c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = temporal_split(feat, train_end=None, val_end=None)\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854a1f5",
   "metadata": {},
   "source": [
    "## 4) SARIMA (accéléré)\n",
    "\n",
    "On **ne change pas SARIMA**. Il est accéléré via un `stride` : on ne fait pas une prévision à chaque heure.\n",
    "\n",
    "- `SARIMA_STRIDE=1` : rolling complet (très lent)\n",
    "- `SARIMA_STRIDE=24` : 1 forecast par jour (rapide)\n",
    "\n",
    "On comparera le LSTM sur les **mêmes origines**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a636ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "SARIMA_STRIDE = 24\n",
    "\n",
    "train_series = train_df[\"load\"]\n",
    "test_series = test_df[\"load\"]\n",
    "\n",
    "sarima_y_true, sarima_y_pred = sarima_rolling_forecast(\n",
    "    train_series=train_series,\n",
    "    test_series=test_series,\n",
    "    order=(1, 0, 1),\n",
    "    seasonal_order=(1, 0, 1, 24),\n",
    "    horizon=HORIZON,\n",
    "    warm_start=INPUT_WIDTH,\n",
    "    stride=SARIMA_STRIDE,\n",
    "    max_points=None,\n",
    "    max_steps=None,\n",
    ")\n",
    "\n",
    "sarima_metrics = evaluate_forecasts(sarima_y_true, sarima_y_pred)\n",
    "sarima_y_true.shape, sarima_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e18365",
   "metadata": {},
   "source": [
    "## 5) LSTM (meilleure pratique, sans toucher à SARIMA)\n",
    "\n",
    "Améliorations par rapport à une version “simple” :\n",
    "\n",
    "- on inclut **la charge passée** dans les features d’entrée (historique observé) en plus des features exogènes\n",
    "- entraînement avec **validation**, **early stopping**, **ReduceLROnPlateau**, **gradient clipping**\n",
    "- on recharge le **meilleur modèle** (val loss minimale)\n",
    "\n",
    "On garde l’architecture LSTM du projet (`src/models.py`) pour rester cohérent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49716bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features LSTM\n",
    "# On inclut la charge 'load' dans X (historique), ce qui est standard en forecasting.\n",
    "exo_cols = [c for c in feat.columns if c != \"load\"]\n",
    "input_feature_cols = [\"load\"] + exo_cols\n",
    "\n",
    "# Scalers (train uniquement)\n",
    "scaler_exo = StandardScaler()\n",
    "scaler_load = StandardScaler()\n",
    "\n",
    "scaler_exo.fit(train_df[exo_cols].to_numpy())\n",
    "scaler_load.fit(train_df[[\"load\"]].to_numpy())\n",
    "\n",
    "def apply_scalers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[exo_cols] = scaler_exo.transform(df[exo_cols].to_numpy())\n",
    "    out[\"load\"] = scaler_load.transform(df[[\"load\"]].to_numpy())\n",
    "    return out\n",
    "\n",
    "train_scaled = apply_scalers(train_df)\n",
    "val_scaled = apply_scalers(val_df)\n",
    "test_scaled = apply_scalers(test_df)\n",
    "\n",
    "# Séquences\n",
    "X_train, y_train = make_sequences(train_scaled, \"load\", input_width=INPUT_WIDTH, forecast_horizon=HORIZON, feature_cols=input_feature_cols)\n",
    "X_val, y_val = make_sequences(val_scaled, \"load\", input_width=INPUT_WIDTH, forecast_horizon=HORIZON, feature_cols=input_feature_cols)\n",
    "X_test, y_test = make_sequences(test_scaled, \"load\", input_width=INPUT_WIDTH, forecast_horizon=HORIZON, feature_cols=input_feature_cols)\n",
    "\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement LSTM (avec validation + early stopping)\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 25\n",
    "PATIENCE = 7\n",
    "LR = 1e-3\n",
    "HIDDEN_SIZE = 64\n",
    "CLIP_NORM = 1.0\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32)),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)),\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "def run_epoch(loader, train: bool):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    losses = []\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "                optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return float(np.mean(losses))\n",
    "\n",
    "model = LSTMForecaster(\n",
    "    input_dim=len(input_feature_cols),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=1,\n",
    "    horizon=HORIZON,\n",
    "    dropout=0.1,\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=2)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "patience_ctr = 0\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    train_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    lr_now = optimizer.param_groups[0][\"lr\"]\n",
    "    history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"lr\": lr_now})\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{MAX_EPOCHS} | train={train_loss:.4f} | val={val_loss:.4f} | lr={lr_now:.2e}\")\n",
    "\n",
    "    if val_loss < best_val - 1e-6:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        patience_ctr = 0\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "        if patience_ctr >= PATIENCE:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "hist_df = pd.DataFrame(history)\n",
    "hist_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courbes d'apprentissage\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"], label=\"train\")\n",
    "ax.plot(hist_df[\"epoch\"], hist_df[\"val_loss\"], label=\"val\")\n",
    "ax.set_title(\"LSTM — loss (scaled)\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16efca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions LSTM sur tout le test, puis sous-échantillonnage pour matcher SARIMA_STRIDE\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_scaled = model(torch.tensor(X_test, dtype=torch.float32).to(DEVICE)).cpu().numpy()\n",
    "\n",
    "# inversion du scaling (cible)\n",
    "\n",
    "def invert_load(arr: np.ndarray) -> np.ndarray:\n",
    "    flat = arr.reshape(-1, 1)\n",
    "    inv = scaler_load.inverse_transform(flat).reshape(arr.shape)\n",
    "    return inv\n",
    "\n",
    "lstm_y_true_full = invert_load(y_test)\n",
    "lstm_y_pred_full = invert_load(preds_scaled)\n",
    "\n",
    "# mêmes origines que SARIMA (stride)\n",
    "sel = np.arange(0, len(lstm_y_true_full), SARIMA_STRIDE)\n",
    "sel = sel[: len(sarima_y_true)]\n",
    "\n",
    "lstm_y_true = lstm_y_true_full[sel]\n",
    "lstm_y_pred = lstm_y_pred_full[sel]\n",
    "\n",
    "# alignement final\n",
    "n = min(len(sarima_y_true), len(lstm_y_true))\n",
    "sarima_y_true = sarima_y_true[:n]\n",
    "sarima_y_pred = sarima_y_pred[:n]\n",
    "lstm_y_true = lstm_y_true[:n]\n",
    "lstm_y_pred = lstm_y_pred[:n]\n",
    "\n",
    "lstm_metrics = evaluate_forecasts(lstm_y_true, lstm_y_pred)\n",
    "lstm_y_true.shape, lstm_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835bd388",
   "metadata": {},
   "source": [
    "## 6) Comparaison SARIMA vs LSTM\n",
    "\n",
    "Métriques + graphes :\n",
    "\n",
    "- bar chart des métriques globales\n",
    "- MAE par horizon\n",
    "- True vs prediction (horizon 1)\n",
    "- exemple de prévision 24h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c955599",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = pd.DataFrame({\n",
    "    \"SARIMA\": evaluate_forecasts(sarima_y_true, sarima_y_pred),\n",
    "    \"LSTM\": evaluate_forecasts(lstm_y_true, lstm_y_pred),\n",
    "}).T\n",
    "model_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"MAE\", \"RMSE\", \"MAPE\"]\n",
    "ax = model_compare[metrics].plot(kind=\"bar\", figsize=(8, 4), rot=0)\n",
    "ax.set_title(f\"{COUNTRY} — Comparaison SARIMA vs LSTM — stride={SARIMA_STRIDE}\")\n",
    "ax.set_ylabel(\"Valeur\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4a1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = np.arange(1, HORIZON + 1)\n",
    "mae_h_sarima = np.array([np.mean(np.abs(sarima_y_true[:, i] - sarima_y_pred[:, i])) for i in range(HORIZON)])\n",
    "mae_h_lstm = np.array([np.mean(np.abs(lstm_y_true[:, i] - lstm_y_pred[:, i])) for i in range(HORIZON)])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(h, mae_h_sarima, marker=\"o\", label=\"SARIMA\")\n",
    "ax.plot(h, mae_h_lstm, marker=\"x\", label=\"LSTM\")\n",
    "ax.set_title(f\"{COUNTRY} — MAE par horizon\")\n",
    "ax.set_xlabel(\"Horizon (heures)\")\n",
    "ax.set_ylabel(\"MAE\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2e4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True vs pred (horizon 1) sur les origines évaluées\n",
    "idx_test = test_df.index\n",
    "origin_idx = INPUT_WIDTH + np.arange(0, n * SARIMA_STRIDE, SARIMA_STRIDE)\n",
    "times = idx_test[origin_idx]\n",
    "\n",
    "true_1 = sarima_y_true[:, 0]\n",
    "sarima_pred_1 = sarima_y_pred[:, 0]\n",
    "lstm_pred_1 = lstm_y_pred[:, 0]\n",
    "\n",
    "window = min(len(times), 14)\n",
    "sl = slice(max(0, len(times) - window), len(times))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(times[sl], true_1[sl], label=\"True\", linewidth=2)\n",
    "ax.plot(times[sl], sarima_pred_1[sl], label=\"SARIMA (t+1)\", alpha=0.8)\n",
    "ax.plot(times[sl], lstm_pred_1[sl], label=\"LSTM (t+1)\", alpha=0.8)\n",
    "ax.set_title(f\"{COUNTRY} — True vs prediction (horizon 1) — stride={SARIMA_STRIDE}\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.set_ylabel(\"Load\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de prévision 24h (un point)\n",
    "sample_idx = min(10, n - 1)\n",
    "hrs = np.arange(1, HORIZON + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(hrs, sarima_y_true[sample_idx], label=\"True\", marker=\"o\")\n",
    "ax.plot(hrs, sarima_y_pred[sample_idx], label=\"SARIMA\", marker=\"x\")\n",
    "ax.plot(hrs, lstm_y_pred[sample_idx], label=\"LSTM\", marker=\".\")\n",
    "ax.set_title(f\"{COUNTRY} — Prévision 24h (exemple)\")\n",
    "ax.set_xlabel(\"Horizon (heures)\")\n",
    "ax.set_ylabel(\"Load\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25193fe1",
   "metadata": {},
   "source": [
    "## 7) Analyse d'erreurs\n",
    "\n",
    "MAE par heure de la journée (horizon 1) pour repérer les périodes difficiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_err = pd.DataFrame({\n",
    "    \"ts\": times,\n",
    "    \"true\": true_1,\n",
    "    \"sarima\": sarima_pred_1,\n",
    "    \"lstm\": lstm_pred_1,\n",
    "})\n",
    "df_err[\"hour\"] = pd.to_datetime(df_err[\"ts\"]).dt.hour\n",
    "\n",
    "df_err[\"abs_err_sarima\"] = (df_err[\"true\"] - df_err[\"sarima\"]).abs()\n",
    "df_err[\"abs_err_lstm\"] = (df_err[\"true\"] - df_err[\"lstm\"]).abs()\n",
    "\n",
    "mae_by_hour = df_err.groupby(\"hour\")[[\"abs_err_sarima\", \"abs_err_lstm\"]].mean()\n",
    "mae_by_hour.columns = [\"SARIMA\", \"LSTM\"]\n",
    "\n",
    "ax = mae_by_hour.plot(kind=\"bar\", figsize=(10, 4), rot=0)\n",
    "ax.set_title(f\"{COUNTRY} — MAE par heure (horizon 1) — stride={SARIMA_STRIDE}\")\n",
    "ax.set_xlabel(\"Heure\")\n",
    "ax.set_ylabel(\"MAE\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea882e8",
   "metadata": {},
   "source": [
    "## Transfert inter-pays (train sur un pays, test sur un autre)\n",
    "\n",
    "Au-delà d’un test *in-country*, on peut évaluer la capacité de généralisation du modèle en le **transférant** :\n",
    "- on entraîne le LSTM sur un pays source (ex: FR),\n",
    "- puis on teste sur un pays cible (ex: DE) avec les mêmes features communes.\n",
    "\n",
    "Dans ce projet, le script `src/train_transfer.py` automatise ce protocole (alignement des features, scaling appris sur le pays source, puis évaluation sur le pays cible).\n",
    "\n",
    "### Reproduire depuis le terminal\n",
    "\n",
    "Exemple (FR → DE) :\n",
    "```bash\n",
    "python -m src.train_transfer --train-country FR --test-country DE --input-width 24 --forecast-horizon 24\n",
    "```\n",
    "\n",
    "Le script écrit les résultats dans `results/transfer_<TRAIN>_to_<TEST>/` :\n",
    "- `metrics_transfer.csv` (métriques globales in-country vs transfer)\n",
    "- `metrics_per_horizon_in_country.csv` et `metrics_per_horizon_transfer.csv`\n",
    "- `error_breakdown_*.csv` (MAE par heure/jour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from IPython.display import display  # type: ignore\n",
    "except Exception:  # pragma: no cover\n",
    "    display = print\n",
    "\n",
    "def load_transfer_results(train_country: str, test_country: str) -> dict:\n",
    "    d = REPO_ROOT / \"results\" / f\"transfer_{train_country}_to_{test_country}\"\n",
    "    if not d.exists():\n",
    "        raise FileNotFoundError(f\"Dossier introuvable: {d}\")\n",
    "    out = {\"dir\": d}\n",
    "    out[\"metrics\"] = pd.read_csv(d / \"metrics_transfer.csv\", index_col=0)\n",
    "    out[\"per_h_in\"] = pd.read_csv(d / \"metrics_per_horizon_in_country.csv\")\n",
    "    out[\"per_h_tr\"] = pd.read_csv(d / \"metrics_per_horizon_transfer.csv\")\n",
    "    # optionnel\n",
    "    in_err = d / \"error_breakdown_in_country.csv\"\n",
    "    tr_err = d / \"error_breakdown_transfer.csv\"\n",
    "    out[\"err_in\"] = pd.read_csv(in_err, index_col=0) if in_err.exists() else None\n",
    "    out[\"err_tr\"] = pd.read_csv(tr_err, index_col=0) if tr_err.exists() else None\n",
    "    return out\n",
    "\n",
    "def plot_transfer_example(train_country: str, test_country: str):\n",
    "    r = load_transfer_results(train_country, test_country)\n",
    "    print(f\"== Transfer {train_country} → {test_country} ==\")\n",
    "    display(r[\"metrics\"])\n",
    "\n",
    "    per_h_in = r[\"per_h_in\"]\n",
    "    per_h_tr = r[\"per_h_tr\"]\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(per_h_in[\"horizon\"], per_h_in[\"MAE\"], marker=\"o\", label=\"Test (même pays que train)\")\n",
    "    ax.plot(per_h_tr[\"horizon\"], per_h_tr[\"MAE\"], marker=\"x\", label=\"Transfer (pays cible)\")\n",
    "    ax.set_title(f\"MAE par horizon — train {train_country}, test {train_country} vs {test_country}\")\n",
    "    ax.set_xlabel(\"Horizon\")\n",
    "    ax.set_ylabel(\"MAE\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if r[\"err_in\"] is not None and r[\"err_tr\"] is not None:\n",
    "        # Un petit aperçu : MAE par heure (transfer vs in-country)\n",
    "        if \"mae_by_hour\" in r[\"err_in\"].columns and \"mae_by_hour\" in r[\"err_tr\"].columns:\n",
    "            df = pd.DataFrame({\n",
    "                \"in_country\": r[\"err_in\"][\"mae_by_hour\"],\n",
    "                \"transfer\": r[\"err_tr\"][\"mae_by_hour\"],\n",
    "            })\n",
    "            ax = df.plot(kind=\"bar\", figsize=(10, 3), rot=0)\n",
    "            ax.set_title(f\"MAE par heure (horizon global) — {train_country} vs {test_country}\")\n",
    "            ax.set_xlabel(\"Heure\")\n",
    "            ax.set_ylabel(\"MAE\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06857cf",
   "metadata": {},
   "source": [
    "### Exemple 1 : FR → DE\n",
    "On observe généralement une dégradation (domain shift) : même architecture et même horizon, mais distribution et patterns différents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b87d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_fr_de = plot_transfer_example(\"FR\", \"DE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3f28f",
   "metadata": {},
   "source": [
    "### Exemple 2 : ES → PT\n",
    "Cas voisin géographiquement : la dégradation peut être plus faible (patterns plus proches), mais elle reste non nulle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a9ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_es_pt = plot_transfer_example(\"SE\", \"NO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbda310",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- Ce notebook met en place une chaîne complète et reproductible : données → EDA → modèles → comparaison → analyse d’erreurs.\n",
    "- **SARIMA** capture bien la saisonnalité (notamment journalière) et fournit un bon point de référence statistique, mais devient coûteux si on évalue des origines trop fréquentes.\n",
    "- **LSTM** (avec historique de charge + validation/regularisation) est plus flexible pour apprendre des non-linéarités et combiner variables exogènes (température, calendrier) et lags.\n",
    "- L’analyse d’erreurs (par heure) permet d’identifier les périodes difficiles, souvent liées aux pics (matin/soir) et aux changements de régime (semaine/weekend).\n",
    "\n",
    "Pistes d’amélioration :\n",
    "- ajuster systématiquement les hyperparamètres (fenêtre d’entrée, hidden size, LR) ;\n",
    "- comparer d’autres architectures (TCN, N-BEATS) ;\n",
    "- enrichir/ablation des variables (météo supplémentaire, jours fériés/vacances plus complets).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
